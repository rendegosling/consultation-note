# Architectural Decisions and Assumptions

## Key Decisions

### 1. Modular Monolith over Microservices
- **Decision**: Chose modular monolith architecture
- **Alternatives Considered**: Microservices, Traditional monolith
- **Rationale**: 
  - Simpler deployment for local development
  - Easier to maintain and test
  - Sufficient for current scale
  - Can be split into microservices later if needed

### 2. PostgreSQL + JSONB
- **Decision**: Use PostgreSQL with JSONB for data storage
- **Alternatives Considered**: MongoDB, MySQL
- **Rationale**:
  - ACID compliance for consultation data
  - Flexible schema with JSONB where needed
  - Strong querying capabilities
  - Excellent community support

### 3. Chunked Audio Processing
- **Decision**: Process audio in chunks asynchronously via API
- **Alternatives Considered**: Real-time streaming, Process audio in chunks asynchronously via buffered SQS, Complete file upload
- **Rationale**:
  - Centralized control over authentication and validation
  - Simpler frontend implementation and error handling
  - Immediate feedback to users through API responses
  - Better handling of network issues through chunk-level retries
  - Easier integration with backend services and storage
  - Simplified monitoring and debugging through backend logs
  - More straightforward path to implement business logic and transformations
  - Reduced complexity compared to direct queue integration

### 4. OpenTelemetry for Observability
- **Decision**: Implement OpenTelemetry from early development
- **Alternatives Considered**: Custom logging, ELK Stack, Datadog
- **Rationale**:
  - Vendor-agnostic solution
  - Works in both local and production environments
  - Comprehensive observability (traces, metrics, logs)
  - Strong community support and standardization

### 5. LLM Simulation Strategy
- **Decision**: Implement LLM simulation with templated responses
- **Alternatives Considered**: 
  - Real LLM service integration
  - Complex simulation with NLP
  - Static response files
- **Rationale**:
  - Simple to implement and test
  - Predictable behavior for development
  - Easy to modify response patterns
  - Simulates realistic timing and errors

### 6. Offline Support Strategy
- **Decision**: Implement basic offline support for note-taking only
- **Alternatives Considered**: 
  - Full offline functionality
  - No offline support
  - Service worker with complete offline cache
- **Rationale**:
  - Notes should be preserved even if connection drops
  - Audio recording requires active connection for chunk upload
  - Simple localStorage-based solution sufficient for demo
  - Balances user experience with implementation complexity

### 8. Audio Processing Strategy
- **Decision**: Implement 15-second chunk-based processing
- **Alternatives Considered**: 
  - Full audio file processing
  - 30-second chunks (too large, higher risk of data loss)
  - 5-second chunks (too frequent, API overhead)
  - Real-time streaming
- **Rationale**:
  - Small enough to minimize data loss risk
  - Large enough for efficient processing
  - Better user experience with more frequent progress updates
  - Good balance for Whisper processing
  - Manageable upload sizes

### 9. Transcription Service
- **Decision**: Use local Whisper (base model) for development
- **Alternatives Considered**: 
  - Cloud services (AWS, Google, Azure)
  - Mock service
  - Larger Whisper models
- **Rationale**:
  - Free and open source
  - Sufficient accuracy for testing
  - Simple Docker deployment
  - Realistic behavior for development

## Key Assumptions (Implementation)

### 1. Scale and Performance
- Local development environment is sufficient for demonstration
- System will handle single-user scenarios effectively
- General API response times under 2 seconds are acceptable
- LLM simulation responses intentionally set to 2-3 seconds to mimic real-world behavior

### 2. Security
- Basic authentication is sufficient for demo
- Data encryption not required in local development
- CORS and basic security headers are adequate

### 3. Data Management
- Audio files will be relatively small (< 1 hour sessions)
- Text data will be minimal
- Temporary storage cleanup is not critical for demo
- LocalStack is sufficient for simulating S3 in development

### 4. Offline Functionality
- Audio recording requires active connection for chunk upload
- Notes can be saved locally when offline
- System will sync notes when connection restores
- Clear user feedback when connection is lost
- Basic error handling for connection recovery

### 5. LLM Simulation
- Response time will be artificially set to 2-3 seconds to simulate realistic LLM processing
- Templates will include placeholders for dynamic content
- Error scenarios will be predefined
- No actual NLP processing required for demo

### Additional Assumptions
#### Audio Processing
- 15-second chunks are optimal for our use case
- Base Whisper model is sufficient for development
- Transcription delay of 0.5-1 second per chunk is acceptable
- Network can handle chunk upload within 3 seconds

## Production Considerations (For Discussion)
These points are reserved for the interview discussion and are not part of the implementation:

### 1. Scaling Considerations
- Multi-user access patterns
- High availability requirements
- Performance optimization needs
- Load balancing strategies
- Rate limiting requirements:
  - Per-user upload limits
  - Chunk size and frequency limits
  - API endpoint throttling
  - DDoS protection considerations

### 2. Security Considerations
- Production authentication requirements
- Data encryption needs
- Audit logging requirements
- Compliance standards

### 3. Operational Considerations
- Monitoring and alerting strategies
- SLA requirements
- Incident response procedures
- Backup and recovery strategies

## Questions for Discussion
- How would this architecture scale in a production environment?
- What security measures would be needed for consultation data?
- How would we handle multi-user scenarios?
- What monitoring would be needed in production?
- How would we handle data retention and compliance requirements?
- What would be the strategy for database migrations in production?
- How would we implement zero-downtime deployments?
- What would be our strategy for transitioning from LLM simulation to real LLM service in production? 